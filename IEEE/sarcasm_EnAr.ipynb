{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_names = ['not_sarcastic', 'sarcastic']\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ•°æ®é›†\n",
    "\n",
    "df_En = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/train.En.csv\")\n",
    "\n",
    "df_Ar = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/train.Ar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø§Ø­Ù„Ù‰ Ø§Ø´ÙŠ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§ ØªÙƒÙˆÙ† Ø¨Ø¹Ù„Ø§Ù‚Ø© Ùˆ ØªØ­Ø³ Ø§Ù†Ùƒ ÙˆØ­ÙŠØ¯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÙˆÙŠÙ†Ø¯ÙˆØ² Ø§Ø­Ø³Ù† Ù†Ø¸Ø§Ù… ØªØ´ØºÙŠÙ„ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø§Ù„Ø§Ù…ÙˆØ± Ù…Ù†ÙŠØ­Ø© Ùˆ Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ø¹Ù„Ù‰ Ù…Ù† Ù‡ÙŠÙƒ ÙØ´</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø´Ø±ÙƒØ© ÙÙŠØ³Ø¨ÙˆÙƒ Ø§ÙƒØ«Ø± Ø´Ø±ÙƒØ© Ø¨Ù‡ØªÙ…ÙˆØ§ Ø¨Ø§Ù„Ø®ØµÙˆØµÙŠØ©</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§Ù„Ø¬Ù‡Ø§Ø² Ù‡ÙˆÙ† ÙƒØ«ÙŠØ± Ù…Ù…ØªØ§Ø² Ùˆ Ù…Ø§ Ø¨Ø¹Ù„Ù‚ Ø§Ø¨Ø¯Ø§ØŒ ØµØ±Ù„ÙŠ Ø³Ø§Ø¹...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>ÙƒÙŠØ±ÙŠ: Ù†Ø¯Ø±Ùƒ Ø£Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ø­Ù„ Ù…Ø³ØªÙ‚Ø± ÙˆØ¯Ø§Ø¦Ù… ÙŠØªØ·Ù„Ø¨ ØªÙ†Ø§...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>Ø­Ù‚Ùƒ ÙƒØ¨ÙŠØ± ğŸ˜³ğŸ˜³Ù‚Ù… Ø¹Ø¯Ù„ Ø§Ù„Ù…Ù†ÙƒØ± .. ÙˆÙ„Ø¯Ùƒ ÙÙŠ Ø®Ø·Ø± !! ÙƒØ±...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>Ø§Ø±ÙØ¶ Ø§Ù„Ø¨Ø°Ø§Ø¡Ø§Øª Ø§Ù„ØªÙ‰ ÙŠØ¨Ø«Ù‡Ø§ Ø§Ù„Ø´ÙŠØ® Ù…Ø­Ù…ÙˆØ¯ Ø´Ø¹Ø¨Ø§Ù† Ø¶Ø¯ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>Ø§Ø­Ù…Ø¯ Ø´ÙÙŠÙ‚ Ù‡Ùˆ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ù‚Ø§Ø¯Ø± Ø¹Ù„ÙŠ ØªØ­Ø±ÙŠÙƒ Ø§Ù„Ø±ÙƒÙˆØ¯ Ø§Ù„...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©: Ù†Ø­Ø§ÙˆÙ„ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ 6 Ù…Ù„Ø§ÙŠÙŠÙ† Ø³ÙˆØ±ÙŠ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2602 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0           Ø§Ø­Ù„Ù‰ Ø§Ø´ÙŠ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§ ØªÙƒÙˆÙ† Ø¨Ø¹Ù„Ø§Ù‚Ø© Ùˆ ØªØ­Ø³ Ø§Ù†Ùƒ ÙˆØ­ÙŠØ¯          1\n",
       "1                        ÙˆÙŠÙ†Ø¯ÙˆØ² Ø§Ø­Ø³Ù† Ù†Ø¸Ø§Ù… ØªØ´ØºÙŠÙ„ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§          1\n",
       "2                 Ø§Ù„Ø§Ù…ÙˆØ± Ù…Ù†ÙŠØ­Ø© Ùˆ Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ø¹Ù„Ù‰ Ù…Ù† Ù‡ÙŠÙƒ ÙØ´          1\n",
       "3                Ø´Ø±ÙƒØ© ÙÙŠØ³Ø¨ÙˆÙƒ Ø§ÙƒØ«Ø± Ø´Ø±ÙƒØ© Ø¨Ù‡ØªÙ…ÙˆØ§ Ø¨Ø§Ù„Ø®ØµÙˆØµÙŠØ©          1\n",
       "4     Ø§Ù„Ø¬Ù‡Ø§Ø² Ù‡ÙˆÙ† ÙƒØ«ÙŠØ± Ù…Ù…ØªØ§Ø² Ùˆ Ù…Ø§ Ø¨Ø¹Ù„Ù‚ Ø§Ø¨Ø¯Ø§ØŒ ØµØ±Ù„ÙŠ Ø³Ø§Ø¹...          1\n",
       "...                                                 ...        ...\n",
       "2597  ÙƒÙŠØ±ÙŠ: Ù†Ø¯Ø±Ùƒ Ø£Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ø­Ù„ Ù…Ø³ØªÙ‚Ø± ÙˆØ¯Ø§Ø¦Ù… ÙŠØªØ·Ù„Ø¨ ØªÙ†Ø§...          0\n",
       "2598   Ø­Ù‚Ùƒ ÙƒØ¨ÙŠØ± ğŸ˜³ğŸ˜³Ù‚Ù… Ø¹Ø¯Ù„ Ø§Ù„Ù…Ù†ÙƒØ± .. ÙˆÙ„Ø¯Ùƒ ÙÙŠ Ø®Ø·Ø± !! ÙƒØ±...          0\n",
       "2599  Ø§Ø±ÙØ¶ Ø§Ù„Ø¨Ø°Ø§Ø¡Ø§Øª Ø§Ù„ØªÙ‰ ÙŠØ¨Ø«Ù‡Ø§ Ø§Ù„Ø´ÙŠØ® Ù…Ø­Ù…ÙˆØ¯ Ø´Ø¹Ø¨Ø§Ù† Ø¶Ø¯ ...          0\n",
       "2600  Ø§Ø­Ù…Ø¯ Ø´ÙÙŠÙ‚ Ù‡Ùˆ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ù‚Ø§Ø¯Ø± Ø¹Ù„ÙŠ ØªØ­Ø±ÙŠÙƒ Ø§Ù„Ø±ÙƒÙˆØ¯ Ø§Ù„...          0\n",
       "2601  Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©: Ù†Ø­Ø§ÙˆÙ„ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ 6 Ù…Ù„Ø§ÙŠÙŠÙ† Ø³ÙˆØ±ÙŠ ...          0\n",
       "\n",
       "[2602 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ar = df_Ar.drop(['id','rephrase','dialect'],axis=1)\n",
    "df_Ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ar = df_Ar[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1000 sarcastic 500 non sarcastic 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_Ar.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "As_train = 0\n",
    "Ans_train = 0\n",
    "for i in df_Ar.sarcastic:\n",
    "    if i == 1:\n",
    "        As_train+=1\n",
    "    else:\n",
    "        Ans_train+=1\n",
    "l_count = len(df_Ar.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", As_train, \"non sarcastic\", Ans_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_En = df_En[:1734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1734 sarcastic 867 non sarcastic 867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_En.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "s_train = 0\n",
    "ns_train = 0\n",
    "for i in df_En.sarcastic:\n",
    "    if i == 1:\n",
    "        s_train+=1\n",
    "    else:\n",
    "        ns_train+=1\n",
    "l_count = len(df_En.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", s_train, \"non sarcastic\", ns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet, add_special_tokens=True, max_length=self.max_len, \n",
    "            return_token_type_ids=False, pad_to_max_length=True,\n",
    "            return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'tweet_text':tweet,\n",
    "            'input_ids':encoding['input_ids'].flatten(),\n",
    "            'attention_mask':encoding['attention_mask'].flatten(),\n",
    "            'targets':torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I do not like when professors donâ€™t write out ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not â€œforcedâ€ to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesnâ€™t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1729</td>\n",
       "      <td>i just overheard this bunch of children behind...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>1730</td>\n",
       "      <td>started telling my dutch friends about how i'v...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1731</td>\n",
       "      <td>took my dog to the vet today because i thought...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>1732</td>\n",
       "      <td>have to somehow become fluent in Spanish in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1733</td>\n",
       "      <td>complained about my job for weeks but i just c...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  The only thing I got from college is a caffein...   \n",
       "1              1  I love it when professors draw a big question ...   \n",
       "2              2  Remember the hundred emails from companies whe...   \n",
       "3              3  Today my pop-pop told me I was not â€œforcedâ€ to...   \n",
       "4              4  @VolphanCarol @littlewhitty @mysticalmanatee I...   \n",
       "...          ...                                                ...   \n",
       "1729        1729  i just overheard this bunch of children behind...   \n",
       "1730        1730  started telling my dutch friends about how i'v...   \n",
       "1731        1731  took my dog to the vet today because i thought...   \n",
       "1732        1732  have to somehow become fluent in Spanish in th...   \n",
       "1733        1733  complained about my job for weeks but i just c...   \n",
       "\n",
       "      sarcastic                                           rephrase  sarcasm  \\\n",
       "0             1  College is really difficult, expensive, tiring...      0.0   \n",
       "1             1  I do not like when professors donâ€™t write out ...      1.0   \n",
       "2             1  I, at the bare minimum, wish companies actuall...      0.0   \n",
       "3             1  Today my pop-pop told me I was not \"forced\" to...      1.0   \n",
       "4             1  I would say Ted Cruz is an asshole and doesnâ€™t...      1.0   \n",
       "...         ...                                                ...      ...   \n",
       "1729          0                                                NaN      NaN   \n",
       "1730          0                                                NaN      NaN   \n",
       "1731          0                                                NaN      NaN   \n",
       "1732          0                                                NaN      NaN   \n",
       "1733          0                                                NaN      NaN   \n",
       "\n",
       "      irony  satire  understatement  overstatement  rhetorical_question  \n",
       "0       1.0     0.0             0.0            0.0                  0.0  \n",
       "1       0.0     0.0             0.0            0.0                  0.0  \n",
       "2       1.0     0.0             0.0            0.0                  0.0  \n",
       "3       0.0     0.0             0.0            0.0                  0.0  \n",
       "4       0.0     0.0             0.0            0.0                  0.0  \n",
       "...     ...     ...             ...            ...                  ...  \n",
       "1729    NaN     NaN             NaN            NaN                  NaN  \n",
       "1730    NaN     NaN             NaN            NaN                  NaN  \n",
       "1731    NaN     NaN             NaN            NaN                  NaN  \n",
       "1732    NaN     NaN             NaN            NaN                  NaN  \n",
       "1733    NaN     NaN             NaN            NaN                  NaN  \n",
       "\n",
       "[1734 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø§Ø­Ù„Ù‰ Ø§Ø´ÙŠ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§ ØªÙƒÙˆÙ† Ø¨Ø¹Ù„Ø§Ù‚Ø© Ùˆ ØªØ­Ø³ Ø§Ù†Ùƒ ÙˆØ­ÙŠØ¯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÙˆÙŠÙ†Ø¯ÙˆØ² Ø§Ø­Ø³Ù† Ù†Ø¸Ø§Ù… ØªØ´ØºÙŠÙ„ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø§Ù„Ø§Ù…ÙˆØ± Ù…Ù†ÙŠØ­Ø© Ùˆ Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ø¹Ù„Ù‰ Ù…Ù† Ù‡ÙŠÙƒ ÙØ´</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø´Ø±ÙƒØ© ÙÙŠØ³Ø¨ÙˆÙƒ Ø§ÙƒØ«Ø± Ø´Ø±ÙƒØ© Ø¨Ù‡ØªÙ…ÙˆØ§ Ø¨Ø§Ù„Ø®ØµÙˆØµÙŠØ©</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§Ù„Ø¬Ù‡Ø§Ø² Ù‡ÙˆÙ† ÙƒØ«ÙŠØ± Ù…Ù…ØªØ§Ø² Ùˆ Ù…Ø§ Ø¨Ø¹Ù„Ù‚ Ø§Ø¨Ø¯Ø§ØŒ ØµØ±Ù„ÙŠ Ø³Ø§Ø¹...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Ù‚Ø§Ù„Ù‡Ø§ Ø¬Ø³ØªÙ† Ø¨ÙŠØ¨Ø± Ø› Ø§Ù†Ø§ Ù…Ùˆ ÙˆÙ„Ù‡Ø§Ù† Ø§Ù†Ø§ Ø§Ù†Ø§ Ø¯Ù†ÙŠØ§ Ù…...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Ù…ÙˆØ§Ù„ÙŠØ¯ Ø¨Ø±Ø¬  Ø§Ù„Ø£Ø³Ø¯ ØªØ¹Ø¬Ø¨Ùƒ ØµØ¯Ø§Ù‚ØªÙ‡Ù… ÙƒØ«ÙŠØ±Ø§Ù‹</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ù…Ø®ØªÙ„Ø·Ù‡ Ù…Ø§Ø±Ø§Ø­ Ø§ØºÙŠØ¨ ÙˆÙ„Ø§ ÙŠÙˆÙ…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3 Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´ Ù…Ø§Ù†Ø¹ Ø§Ù†Ø§ Ø¨ØªÙƒÙ„Ù… Ø¹Ù† Ø§Ù„ÙØ¬Ø± ÙÙŠ Ø§Ù„Ø®ØµÙˆÙ…Ø©</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Ø§Ù„ØªØ¹Ø§Ø·ÙŠ Ù…Ø¹ Ù…Ù„Ù Ø§Ù„Ù†Ø§Ø²Ø­ÙŠÙ† Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙÙŠ Ù„Ø¨Ù†Ø§Ù† ÙŠÙƒÙˆ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  sarcastic\n",
       "0          Ø§Ø­Ù„Ù‰ Ø§Ø´ÙŠ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§ ØªÙƒÙˆÙ† Ø¨Ø¹Ù„Ø§Ù‚Ø© Ùˆ ØªØ­Ø³ Ø§Ù†Ùƒ ÙˆØ­ÙŠØ¯          1\n",
       "1                       ÙˆÙŠÙ†Ø¯ÙˆØ² Ø§Ø­Ø³Ù† Ù†Ø¸Ø§Ù… ØªØ´ØºÙŠÙ„ Ø¨Ø§Ù„Ø¯Ù†ÙŠØ§          1\n",
       "2                Ø§Ù„Ø§Ù…ÙˆØ± Ù…Ù†ÙŠØ­Ø© Ùˆ Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ø¹Ù„Ù‰ Ù…Ù† Ù‡ÙŠÙƒ ÙØ´          1\n",
       "3               Ø´Ø±ÙƒØ© ÙÙŠØ³Ø¨ÙˆÙƒ Ø§ÙƒØ«Ø± Ø´Ø±ÙƒØ© Ø¨Ù‡ØªÙ…ÙˆØ§ Ø¨Ø§Ù„Ø®ØµÙˆØµÙŠØ©          1\n",
       "4    Ø§Ù„Ø¬Ù‡Ø§Ø² Ù‡ÙˆÙ† ÙƒØ«ÙŠØ± Ù…Ù…ØªØ§Ø² Ùˆ Ù…Ø§ Ø¨Ø¹Ù„Ù‚ Ø§Ø¨Ø¯Ø§ØŒ ØµØ±Ù„ÙŠ Ø³Ø§Ø¹...          1\n",
       "..                                                 ...        ...\n",
       "995   Ù‚Ø§Ù„Ù‡Ø§ Ø¬Ø³ØªÙ† Ø¨ÙŠØ¨Ø± Ø› Ø§Ù†Ø§ Ù…Ùˆ ÙˆÙ„Ù‡Ø§Ù† Ø§Ù†Ø§ Ø§Ù†Ø§ Ø¯Ù†ÙŠØ§ Ù…...          0\n",
       "996             Ù…ÙˆØ§Ù„ÙŠØ¯ Ø¨Ø±Ø¬  Ø§Ù„Ø£Ø³Ø¯ ØªØ¹Ø¬Ø¨Ùƒ ØµØ¯Ø§Ù‚ØªÙ‡Ù… ÙƒØ«ÙŠØ±Ø§Ù‹          0\n",
       "997         Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ù…Ø®ØªÙ„Ø·Ù‡ Ù…Ø§Ø±Ø§Ø­ Ø§ØºÙŠØ¨ ÙˆÙ„Ø§ ÙŠÙˆÙ…           0\n",
       "998       3 Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´ Ù…Ø§Ù†Ø¹ Ø§Ù†Ø§ Ø¨ØªÙƒÙ„Ù… Ø¹Ù† Ø§Ù„ÙØ¬Ø± ÙÙŠ Ø§Ù„Ø®ØµÙˆÙ…Ø©          0\n",
       "999   Ø§Ù„ØªØ¹Ø§Ø·ÙŠ Ù…Ø¹ Ù…Ù„Ù Ø§Ù„Ù†Ø§Ø²Ø­ÙŠÙ† Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙÙŠ Ù„Ø¨Ù†Ø§Ù† ÙŠÙƒÙˆ...          0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not â€œforcedâ€ to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>Ù‚Ø§Ù„Ù‡Ø§ Ø¬Ø³ØªÙ† Ø¨ÙŠØ¨Ø± Ø› Ø§Ù†Ø§ Ù…Ùˆ ÙˆÙ„Ù‡Ø§Ù† Ø§Ù†Ø§ Ø§Ù†Ø§ Ø¯Ù†ÙŠØ§ Ù…...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>Ù…ÙˆØ§Ù„ÙŠØ¯ Ø¨Ø±Ø¬  Ø§Ù„Ø£Ø³Ø¯ ØªØ¹Ø¬Ø¨Ùƒ ØµØ¯Ø§Ù‚ØªÙ‡Ù… ÙƒØ«ÙŠØ±Ø§Ù‹</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ù…Ø®ØªÙ„Ø·Ù‡ Ù…Ø§Ø±Ø§Ø­ Ø§ØºÙŠØ¨ ÙˆÙ„Ø§ ÙŠÙˆÙ…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>3 Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´ Ù…Ø§Ù†Ø¹ Ø§Ù†Ø§ Ø¨ØªÙƒÙ„Ù… Ø¹Ù† Ø§Ù„ÙØ¬Ø± ÙÙŠ Ø§Ù„Ø®ØµÙˆÙ…Ø©</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>Ø§Ù„ØªØ¹Ø§Ø·ÙŠ Ù…Ø¹ Ù…Ù„Ù Ø§Ù„Ù†Ø§Ø²Ø­ÙŠÙ† Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙÙŠ Ù„Ø¨Ù†Ø§Ù† ÙŠÙƒÙˆ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2733 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not â€œforcedâ€ to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "2728   Ù‚Ø§Ù„Ù‡Ø§ Ø¬Ø³ØªÙ† Ø¨ÙŠØ¨Ø± Ø› Ø§Ù†Ø§ Ù…Ùˆ ÙˆÙ„Ù‡Ø§Ù† Ø§Ù†Ø§ Ø§Ù†Ø§ Ø¯Ù†ÙŠØ§ Ù…...          0\n",
       "2729             Ù…ÙˆØ§Ù„ÙŠØ¯ Ø¨Ø±Ø¬  Ø§Ù„Ø£Ø³Ø¯ ØªØ¹Ø¬Ø¨Ùƒ ØµØ¯Ø§Ù‚ØªÙ‡Ù… ÙƒØ«ÙŠØ±Ø§Ù‹          0\n",
       "2730         Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ù…Ø®ØªÙ„Ø·Ù‡ Ù…Ø§Ø±Ø§Ø­ Ø§ØºÙŠØ¨ ÙˆÙ„Ø§ ÙŠÙˆÙ…           0\n",
       "2731       3 Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´ Ù…Ø§Ù†Ø¹ Ø§Ù†Ø§ Ø¨ØªÙƒÙ„Ù… Ø¹Ù† Ø§Ù„ÙØ¬Ø± ÙÙŠ Ø§Ù„Ø®ØµÙˆÙ…Ø©          0\n",
       "2732   Ø§Ù„ØªØ¹Ø§Ø·ÙŠ Ù…Ø¹ Ù…Ù„Ù Ø§Ù„Ù†Ø§Ø²Ø­ÙŠÙ† Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙÙŠ Ù„Ø¨Ù†Ø§Ù† ÙŠÙƒÙˆ...          0\n",
       "\n",
       "[2733 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = pd.concat((df_En, df_Ar), join='outer')\n",
    "\n",
    "df_total = df_total.dropna(subset=['tweet'])\n",
    "df_total =df_total.dropna(axis=1,how='any')\n",
    "df_total =df_total.reset_index(drop=True)\n",
    "df_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clearTextFunc(text):\n",
    "    clear_text= \" \".join(text.split())\n",
    "    clear_text=re.sub(r'http://[a-zA-Z0-9.?/&=:]*',\" \", clear_text) #å»é™¤URL\n",
    "    clear_text=re.sub(r'https://[a-zA-Z0-9.?/&=:]*',\" \", clear_text) #å»é™¤URL\n",
    "    clear_text=re.sub(r'@[a-zA-Z0-9.?/&=:]*',\" \", clear_text)#å»é™¤@\n",
    "    clear_text=re.sub(r'/s',\" \",clear_text)\n",
    "    clear_text=re.sub(r'/j',\" \",clear_text)\n",
    "\n",
    "    return clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not â€œforcedâ€ to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I did too, and I also reported Cancun Cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>Ù‚Ø§Ù„Ù‡Ø§ Ø¬Ø³ØªÙ† Ø¨ÙŠØ¨Ø± Ø› Ø§Ù†Ø§ Ù…Ùˆ ÙˆÙ„Ù‡Ø§Ù† Ø§Ù†Ø§ Ø§Ù†Ø§ Ø¯Ù†ÙŠØ§ Ù…Ù†...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>Ù…ÙˆØ§Ù„ÙŠØ¯ Ø¨Ø±Ø¬ Ø§Ù„Ø£Ø³Ø¯ ØªØ¹Ø¬Ø¨Ùƒ ØµØ¯Ø§Ù‚ØªÙ‡Ù… ÙƒØ«ÙŠØ±Ø§Ù‹</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ù…Ø®ØªÙ„Ø·Ù‡ Ù…Ø§Ø±Ø§Ø­ Ø§ØºÙŠØ¨ ÙˆÙ„Ø§ ÙŠÙˆÙ…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>3 Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´ Ù…Ø§Ù†Ø¹ Ø§Ù†Ø§ Ø¨ØªÙƒÙ„Ù… Ø¹Ù† Ø§Ù„ÙØ¬Ø± ÙÙŠ Ø§Ù„Ø®ØµÙˆÙ…Ø©</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>Ø§Ù„ØªØ¹Ø§Ø·ÙŠ Ù…Ø¹ Ù…Ù„Ù Ø§Ù„Ù†Ø§Ø²Ø­ÙŠÙ† Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙÙŠ Ù„Ø¨Ù†Ø§Ù† ÙŠÙƒÙˆÙ†...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2733 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not â€œforcedâ€ to...          1\n",
       "4           I did too, and I also reported Cancun Cr...          1\n",
       "...                                                 ...        ...\n",
       "2728  Ù‚Ø§Ù„Ù‡Ø§ Ø¬Ø³ØªÙ† Ø¨ÙŠØ¨Ø± Ø› Ø§Ù†Ø§ Ù…Ùˆ ÙˆÙ„Ù‡Ø§Ù† Ø§Ù†Ø§ Ø§Ù†Ø§ Ø¯Ù†ÙŠØ§ Ù…Ù†...          0\n",
       "2729              Ù…ÙˆØ§Ù„ÙŠØ¯ Ø¨Ø±Ø¬ Ø§Ù„Ø£Ø³Ø¯ ØªØ¹Ø¬Ø¨Ùƒ ØµØ¯Ø§Ù‚ØªÙ‡Ù… ÙƒØ«ÙŠØ±Ø§Ù‹          0\n",
       "2730          Ù…Ø§Ø°Ø§ Ù„Ùˆ Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ù…Ø®ØªÙ„Ø·Ù‡ Ù…Ø§Ø±Ø§Ø­ Ø§ØºÙŠØ¨ ÙˆÙ„Ø§ ÙŠÙˆÙ…          0\n",
       "2731       3 Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´ Ù…Ø§Ù†Ø¹ Ø§Ù†Ø§ Ø¨ØªÙƒÙ„Ù… Ø¹Ù† Ø§Ù„ÙØ¬Ø± ÙÙŠ Ø§Ù„Ø®ØµÙˆÙ…Ø©          0\n",
       "2732  Ø§Ù„ØªØ¹Ø§Ø·ÙŠ Ù…Ø¹ Ù…Ù„Ù Ø§Ù„Ù†Ø§Ø²Ø­ÙŠÙ† Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙÙŠ Ù„Ø¨Ù†Ø§Ù† ÙŠÙƒÙˆÙ†...          0\n",
       "\n",
       "[2733 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total['tweet']=df_total['tweet'].apply(clearTextFunc)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 2733 sarcastic 1367 non sarcastic 1366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXv0lEQVR4nO3de7SddX3n8feHIHcRUgITk0wTXZEOoKIEirq8tOiAUxVGpY2jNVZqLItaddUL6Bp1tU1rlzj1il0RlUAdMUtU0q6iYKZIZ7xgQDAEjKSkQiSSQ6UK2gYC3/nj+aVsDyd5DuGcs5Oc92utvfbz/J7L/p7k7PPZv+fy26kqJEnamX2GXYAkafdnWEiSehkWkqRehoUkqZdhIUnqte+wC5gsRxxxRM2fP3/YZUjSHuW66667u6pmjW7fa8Ni/vz5rFmzZthlSNIeJckPx2r3MJQkqdekhUWSTyfZkuSmMZa9LUklOWKg7bwkG5KsT3LqQPsJSda2ZR9JksmqWZI0tsnsWVwEnDa6Mck84EXA7QNtxwCLgWPbNhckmdEWfwJYCixsj0fsU5I0uSYtLKrqGuAnYyz6K+AdwOA4I6cDl1bV1qraCGwATkoyGzi0qr5Z3bgkFwNnTFbNkqSxTek5iyQvA35UVTeOWjQHuGNgflNrm9OmR7fvaP9Lk6xJsmZkZGSCqpYkTVlYJDkIeDfwnrEWj9FWO2kfU1Utr6pFVbVo1qxHXPklSdpFU3np7JOBBcCN7Rz1XOD6JCfR9RjmDaw7F7iztc8do12SNIWmrGdRVWur6siqml9V8+mC4JlV9WNgFbA4yf5JFtCdyL62qjYD9yY5uV0F9Vrg8qmqWZLUmcxLZz8HfBM4OsmmJGftaN2qWgesBG4GvgKcU1UPtsVnAxfSnfT+J+CKyapZkjS27K1ffrRo0aJ6LHdwn/D2iyewGu0trvvAa4ddAgC3/8lTh12CdkP/+T1rH/M+klxXVYtGt3sHtySpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKnXpIVFkk8n2ZLkpoG2DyT5fpLvJflSksMGlp2XZEOS9UlOHWg/IcnatuwjSTJZNUuSxjaZPYuLgNNGtV0FHFdVTwN+AJwHkOQYYDFwbNvmgiQz2jafAJYCC9tj9D4lSZNs0sKiqq4BfjKq7cqq2tZmvwXMbdOnA5dW1daq2ghsAE5KMhs4tKq+WVUFXAycMVk1S5LGNsxzFq8HrmjTc4A7BpZtam1z2vTo9jElWZpkTZI1IyMjE1yuJE1fQwmLJO8GtgGf3d40xmq1k/YxVdXyqlpUVYtmzZr12AuVJAGw71S/YJIlwEuAU9qhJeh6DPMGVpsL3Nna547RLkmaQlPas0hyGvBO4GVV9YuBRauAxUn2T7KA7kT2tVW1Gbg3ycntKqjXApdPZc2SpEnsWST5HPAC4Igkm4D30l39tD9wVbsC9ltV9QdVtS7JSuBmusNT51TVg21XZ9NdWXUg3TmOK5AkTalJC4uqetUYzZ/ayfrLgGVjtK8BjpvA0iRJj5J3cEuSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6TVpYJPl0ki1Jbhpom5nkqiS3tufDB5adl2RDkvVJTh1oPyHJ2rbsI0kyWTVLksY2mT2Li4DTRrWdC6yuqoXA6jZPkmOAxcCxbZsLksxo23wCWAosbI/R+5QkTbJJC4uqugb4yajm04EVbXoFcMZA+6VVtbWqNgIbgJOSzAYOrapvVlUBFw9sI0maIlN9zuKoqtoM0J6PbO1zgDsG1tvU2ua06dHtY0qyNMmaJGtGRkYmtHBJms52lxPcY52HqJ20j6mqllfVoqpaNGvWrAkrTpKmu6kOi7vaoSXa85bWvgmYN7DeXODO1j53jHZJ0hSa6rBYBSxp00uAywfaFyfZP8kCuhPZ17ZDVfcmObldBfXagW0kSVNk38nacZLPAS8AjkiyCXgv8H5gZZKzgNuBMwGqal2SlcDNwDbgnKp6sO3qbLorqw4ErmgPSdIUmrSwqKpX7WDRKTtYfxmwbIz2NcBxE1iaJOlR2l1OcEuSdmOGhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXkMJiyRvTbIuyU1JPpfkgCQzk1yV5Nb2fPjA+ucl2ZBkfZJTh1GzJE1nUx4WSeYAfwQsqqrjgBnAYuBcYHVVLQRWt3mSHNOWHwucBlyQZMZU1y1J09mwDkPtCxyYZF/gIOBO4HRgRVu+AjijTZ8OXFpVW6tqI7ABOGlqy5Wk6W3Kw6KqfgScD9wObAZ+WlVXAkdV1ea2zmbgyLbJHOCOgV1sam2SpCkyjMNQh9P1FhYATwQOTvKanW0yRlvtYN9Lk6xJsmZkZOSxFytJAsYZFklWj6dtnF4IbKyqkap6APgi8GzgriSz275nA1va+puAeQPbz6U7bPUIVbW8qhZV1aJZs2btYnmSpNF2Ghbbr1ICjkhyeLtiaWaS+XS9gl1xO3BykoOSBDgFuAVYBSxp6ywBLm/Tq4DFSfZPsgBYCFy7i68tSdoF+/YsfyPwFrpguI6HDwn9DPj4rrxgVX07yReA64FtwHeB5cAhwMokZ9EFyplt/XVJVgI3t/XPqaoHd+W1JUm7ZqdhUVUfBj6c5E1V9dGJetGqei/w3lHNW+l6GWOtvwxYNlGvL0l6dPp6FgBU1UeTPBuYP7hNVV08SXVJknYj4wqLJJcATwZuALYfAirAsJCkaWBcYQEsAo6pqjEvWZUk7d3Ge5/FTcB/msxCJEm7r/H2LI4Abk5yLd2JaACq6mWTUpUkabcy3rB432QWIUnavY33aqivT3YhkqTd13ivhrqXh8dj2g94HPDzqjp0sgqTJO0+xtuzePzgfJIzcJhwSZo2dmnU2ar6MvCbE1uKJGl3Nd7DUC8fmN2H7r4L77mQpGlivFdDvXRgehvwz3TfSSFJmgbGe87i9ya7EEnS7mu8X340N8mXkmxJcleSy5LMneziJEm7h/Ge4P4M3ZcQPZHu+6//trVJkqaB8YbFrKr6TFVta4+LAL+3VJKmifGGxd1JXpNkRnu8BviXySxMkrT7GG9YvB74beDHwGbglYAnvSVpmhjvpbN/CiypqnsAkswEzqcLEUnSXm68PYunbQ8KgKr6CfCMySlJkrS7GW9Y7JPk8O0zrWcx3l6JJGkPN94/+B8EvpHkC3TDfPw2sGzSqpIk7VbG1bOoqouBVwB3ASPAy6vqkl190SSHJflCku8nuSXJs5LMTHJVklvb82BP5rwkG5KsT3Lqrr6uJGnXjPtQUlXdDNw8Qa/7YeArVfXKJPsBBwHvAlZX1fuTnAucC7wzyTHAYuBYupsCv5bkKVX14ATVIknqsUtDlD8WSQ4Fngd8CqCq7q+qf6UbmHBFW20FcEabPh24tKq2VtVGYAN+l4YkTakpDwvgSXSHsj6T5LtJLkxyMHBUVW0GaM9HtvXnAHcMbL+ptT1CkqVJ1iRZMzIyMnk/gSRNM8MIi32BZwKfqKpnAD+nO+S0Ixmjbczv0qiq5VW1qKoWzZrlaCSSNFGGERabgE1V9e02/wW68LgryWyA9rxlYP15A9vPBe6cololSQwhLKrqx8AdSY5uTafQnThfBSxpbUuAy9v0KmBxkv2TLAAWAtdOYcmSNO0N68a6NwGfbVdC3UY3ztQ+wMokZwG3A2cCVNW6JCvpAmUbcI5XQknS1BpKWFTVDXTf4z3aKTtYfxneBChJQzOMcxaSpD2MYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqdfQwiLJjCTfTfJ3bX5mkquS3NqeDx9Y97wkG5KsT3LqsGqWpOlqmD2LNwO3DMyfC6yuqoXA6jZPkmOAxcCxwGnABUlmTHGtkjStDSUskswFfgu4cKD5dGBFm14BnDHQfmlVba2qjcAG4KQpKlWSxPB6Fh8C3gE8NNB2VFVtBmjPR7b2OcAdA+ttam2PkGRpkjVJ1oyMjEx40ZI0XU15WCR5CbClqq4b7yZjtNVYK1bV8qpaVFWLZs2atcs1SpJ+2b5DeM3nAC9L8t+AA4BDk/wNcFeS2VW1OclsYEtbfxMwb2D7ucCdU1qxJE1zU96zqKrzqmpuVc2nO3H9f6rqNcAqYElbbQlweZteBSxOsn+SBcBC4NopLluSprVh9Cx25P3AyiRnAbcDZwJU1bokK4GbgW3AOVX14PDKlKTpZ6hhUVVXA1e36X8BTtnBesuAZVNWmCTpl3gHtySpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKnXlIdFknlJ/iHJLUnWJXlza5+Z5Kokt7bnwwe2OS/JhiTrk5w61TVL0nQ3jJ7FNuCPq+q/ACcD5yQ5BjgXWF1VC4HVbZ62bDFwLHAacEGSGUOoW5KmrSkPi6raXFXXt+l7gVuAOcDpwIq22grgjDZ9OnBpVW2tqo3ABuCkKS1akqa5oZ6zSDIfeAbwbeCoqtoMXaAAR7bV5gB3DGy2qbVJkqbI0MIiySHAZcBbqupnO1t1jLbawT6XJlmTZM3IyMhElClJYkhhkeRxdEHx2ar6Ymu+K8nstnw2sKW1bwLmDWw+F7hzrP1W1fKqWlRVi2bNmjU5xUvSNDSMq6ECfAq4par+18CiVcCSNr0EuHygfXGS/ZMsABYC105VvZIk2HcIr/kc4HeBtUluaG3vAt4PrExyFnA7cCZAVa1LshK4me5KqnOq6sEpr1qSprEpD4uq+r+MfR4C4JQdbLMMWDZpRUmSdso7uCVJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUq89JiySnJZkfZINSc4ddj2SNJ3sEWGRZAbwceDFwDHAq5IcM9yqJGn62CPCAjgJ2FBVt1XV/cClwOlDrkmSpo19h13AOM0B7hiY3wT8+uiVkiwFlrbZ+5Ksn4LapoMjgLuHXcTuIOcvGXYJeiR/P7d7byZiL786VuOeEhZj/QvUIxqqlgPLJ7+c6SXJmqpaNOw6pLH4+zk19pTDUJuAeQPzc4E7h1SLJE07e0pYfAdYmGRBkv2AxcCqIdckSdPGHnEYqqq2JflD4KvADODTVbVuyGVNJx7a0+7M388pkKpHHPqXJOmX7CmHoSRJQ2RYSJJ6GRaSpoUk7xo1/41h1bInMiz2Qklel+SJw65jZ0bXmORCh3DRWJJM1IU4vxQWVfXsCdrvtLBHXA2lR+11wE1Mwr0oSfatqm0TsKvXMVBjVf3+BOxTu7EkBwMr6e6TmgH8KXA08FLgQOAbwBurqpJc3eafA6xKcg3wYeBgYCtwCvArwCWtDeAPq+obSWYDnwcOpfsbdzbwW8CBSW4A1lXVq5PcV1WHtNreAfwu8BBwRVU5WOloVeVjN38A84FbgE8C64Ar6d5cxwPfAr4HfAk4HHglcB+wHrgBOHAH+3w/cHPb9vzW9lLg28B3ga8BR7X299Fdnngl8L+Bo9rr3dgez27rfRm4rtW4tLXNAC6iC4a1wFvHqhG4GljUtjkNuL7te/Ww//19TNjv8SuATw7MPwGYOTB/CfDSNn01cEGb3g+4DTixzW8PgYOAA1rbQmBNm/5j4N0Dv3+Pb9P3jarnvvb8YrpgOqjNz3ysP+ve+Bh6AT7G8Z/UhcU24Pg2vxJ4TftD//zW9ifAh9r0f/zh3cH+ZrY/1NsvnT6sPR8+0Pb7wAfb9PtaCBzY5j8PvKVNzwCesH2/7fnAFg6/ApwAXDXw2oeNVeP2eWAW3ThgCwb36WPPfwBPATYCfwk8t7W9gu4DylrgR8C5A78Pz2/TTwX+3xj7e0ILmLV0Hzp+0dqfB2xov7fHD6y/o7D4IPCGYf/77O4Pz1nsOTZW1Q1t+jrgyXR/eL/e2lbQvUnG42fAvwMXJnk58IvWPhf4apK1wNuBYwe2WVVV/9amfxP4BEBVPVhVP23tf5TkRrrezjy6T3u3AU9K8tEkp7XX3pmTgWuqamPb/0/G+TNpN1dVP6D78LAW+Isk7wEuAF5ZVU+l6zkfMLDJz9tzGGMsOLpe6l3A0+k+aOzXXucauvfCj4BLkry2p7Qd7V8DDIs9x9aB6QeBw3Z1R9WdczgJuAw4A/hKW/RR4GPtjftGxn7jjinJC4AXAs+qqqfTHco6oKruoXszXw2cA1zYU55v3L1Uu6DhF1X1N8D5wDPboruTHEJ3eHIs3weemOTEtp/Ht5PeTwA2V9VDdOcbZrTlvwpsqapPAp8aeJ0HkjxujP1fCbw+yUFt+5mP8UfdK3mCe8/1U+CeJM+tqn+ke7Ns72XcCzx+Rxu2N+ZBVfX3Sb5F12WH7s33oza9s7G4V9OdNPxQ+2Kqg9u291TVL5L8Gl0PgSRHAPdX1WVJ/onu/MXOavwm8PEkC6pqY5KZ9i72Gk8FPpDkIeABut+hM+h6Gv9MNwbcI1TV/Ul+B/hokgOBf6P7YHIBcFmSM4F/4OEPNC8A3p7kAbpzY9t7FsuB7yW5vqpePbD/ryQ5HliT5H7g7xl15ZQc7mOPkGQ+8HdVdVybfxtwCN0J5b+mO9F3G/B7VXVPklcAf073pnrWwOGj7fubDVxO13MI3QnuFUlOB/6KLjC+RXdC8QVJ3kd3fPf8tv1RdG+8J9H1cs6mOyH9ZbrvHllPd+7hfcA9wGd4uBd7XlVdMbpG4ArgbVW1JsmL27J96D4hvuix/ytKeiwMC0lSL89ZSJJ6ec5iL5fkS8CCUc3vrKqvDqMeSXsmD0NJknp5GEqS1MuwkCT1Miw0LSR5d5J1Sb6X5IYkvz7smvokmZ/kpkex/kVJdnRj22Pev6Y3T3Brr5fkWcBLgGdW1dZ2o+B+j2L7iRppV9pj2bPQdDAbuLuqtgJU1d1VdSdAkvck+U6Sm5IsT5LWfnWSP0/ydeDNSU5M8o0kNya5tg05MT/JPya5vj2e3badneSa1oO5KclzW/t9Sf4yyXVJvpbkpPY6tyV52Xh/mCRvaDXfmOSy7cNUNC9sNf0gyUva+jOSfKBt870kb5yQf1VNK4aFpoMrgXntD+gFSZ4/sOxjVXViuzv+QLoeyHaHVdXz6cbM+jzw5jbu1Qvp7jzfAryoqp4J/A7wkbbd/wC+WlXH042LdUNrPxi4uqpOoBvu5M+AFwH/nW7U4PH6Yqv56XRD1581sGw+8Hy672/46yQHtOU/raoTgROBNyQZfTm1tFMehtJer6ruS3IC8FzgN4DPJzm3qi4CfqN98c1BdEO3rwP+tm36+fZ8NN2Add9p+/sZ/MeX+XysjSv0IN0Q3NCNcfTpNmjdlwdGC76fhwdtXAtsraoH2ii/8x/Fj3Rckj+jG0zyEGDwnpmVbWC9W5PcBvwa8F+Bpw2cz3gC3YjAP3gUr6lpzrDQtFBVD9KNfHt1++O8JMmldIPRLaqqO9oYWLs6RPY+dMO+U1XXJHke3af7S5J8oKouBh6oh29seog2knBVPZRH99WhFwFnVNWNSV5HN3Def/yoo3/0Vv+bRt+I2cYck8bFw1Da6yU5OsnCgabjgR/ycDBM1RDZE+XxwObWc3n1qGVnJtknyZPpBnpcT9fzOHv78NxJntJ6RdK42bPQdHAI3fDWh9F94+AGuq99/dckn2TqhsjeFUcn2TQw/1bgf9J9u9wPW+2DQ72vpxuq/ijgD6rq35NcSHeY6/p2An+Ebmhwadwc7kOS1MvDUJKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSer1/wE6RRbKTCCYSwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_total.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "ts_train = 0\n",
    "tns_train = 0\n",
    "for i in df_total.sarcastic:\n",
    "    if i == 1:\n",
    "        ts_train+=1\n",
    "    else:\n",
    "        tns_train+=1\n",
    "l_count = len(df_total.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", ts_train, \"non sarcastic\", tns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=RANDOM_SEED)\n",
    "for train_index, text_index in split.split(df_total, df_total['sarcastic']):\n",
    "    df_train = df_total.loc[train_index]\n",
    "    df_val = df_total.loc[text_index]\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_Warner     Not sure if this has been answe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This second jab has made me feel off the box t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My dad is bringing me home Spider-Man Vans fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¬Ù…Ø§Ù„ Ø³Ù„Ø·Ø§Ù† ÙØ¶Ø­ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ ÙˆØ§Ø¹Ø§Ø¯ Ø§Ù„Ø­Ù‚ Ù„Ù…Ø±Ø³ÙŠ Ø¨Ø¹Ø¯ Ø§...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Snowden Sad to see intelligent Americans fall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ Ù†Ù‡Ø§ÙŠØªÙ‡ 'Ù…Ø²Ø¨Ù„Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>Iâ€™m at a weird point in my life where Iâ€™m on e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>Ø±ÙˆØ­ ÙŠØ§ Ø±Ø§Ø¬Ù„ Ø´ÙˆÙÙ„Ùƒ Ø­ØªÙ‡ Ù†Ø§Ø´ÙØ© Ø§Ù‚Ø¹Ø¯ ÙÙŠÙ‡Ø§</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ø¹Ø§Ø·Ù‰ Ø§Ù„ÙˆÙ‡Ø§Ø¨ Ù…Ù† Ø¨Ø¹Ø¯ Ø§Ù„Ø´Ø¨Ø´Ø¨ ÙˆØ§Ù„Ø§ÙˆØ¨Ø¦Ø§Ø¨</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>If they arenâ€™t out yet just wait to get the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1913 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0        _Warner     Not sure if this has been answe...          0\n",
       "1     This second jab has made me feel off the box t...          0\n",
       "2     My dad is bringing me home Spider-Man Vans fro...          0\n",
       "3     Ø¬Ù…Ø§Ù„ Ø³Ù„Ø·Ø§Ù† ÙØ¶Ø­ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ ÙˆØ§Ø¹Ø§Ø¯ Ø§Ù„Ø­Ù‚ Ù„Ù…Ø±Ø³ÙŠ Ø¨Ø¹Ø¯ Ø§...          0\n",
       "4     #Snowden Sad to see intelligent Americans fall...          0\n",
       "...                                                 ...        ...\n",
       "1908                     Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ Ù†Ù‡Ø§ÙŠØªÙ‡ 'Ù…Ø²Ø¨Ù„Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®'          0\n",
       "1909  Iâ€™m at a weird point in my life where Iâ€™m on e...          0\n",
       "1910              Ø±ÙˆØ­ ÙŠØ§ Ø±Ø§Ø¬Ù„ Ø´ÙˆÙÙ„Ùƒ Ø­ØªÙ‡ Ù†Ø§Ø´ÙØ© Ø§Ù‚Ø¹Ø¯ ÙÙŠÙ‡Ø§          1\n",
       "1911        Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ø¹Ø§Ø·Ù‰ Ø§Ù„ÙˆÙ‡Ø§Ø¨ Ù…Ù† Ø¨Ø¹Ø¯ Ø§Ù„Ø´Ø¨Ø´Ø¨ ÙˆØ§Ù„Ø§ÙˆØ¨Ø¦Ø§Ø¨          1\n",
       "1912    If they arenâ€™t out yet just wait to get the ...          0\n",
       "\n",
       "[1913 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i want tech to be banned for use in work relat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy birthday gorgeous! have an amazing day i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you to   for organising pizza and ice cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§Ø­Ù„Ù‰ Ø£ØºÙ†ÙŠØ© ØªÙŠØªØ± ÙÙŠ Ø±Ù…Ø¶Ø§Ù† Ø§ØºÙ†ÙŠØ© Ø´ÙŠØ±ÙŠÙ† - Ù…Ø´Ø§Ø¹Ø± Ù…...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø¨ÙŠØ´ØªØºÙ„ ØªÙ„Ø§Øª Ø³Ø§Ø¹Ø§Øª ÙˆÙŠØ±Ø¬Ø¹ ÙŠÙ‚ÙˆÙ„ ØªØ¹Ø¨Ø§Ù† Ù…Ø´ Ù‚Ø§Ø¯Ø±</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>ÙˆØ­ÙŠØ§Øª Ø¹ÙŠÙ†ÙŠÙƒ ØŒØŒØŒØŒ Ø§Ø®Ø° ØµÙˆØª Ø§Ø®Ø° ÙˆØ¬Ù‡ Ø­Ø³Ù† ÙˆØ·Ø±Ø¨ Ø§ØµÙŠÙŠ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>The most 2020 quote of the year: Me: we should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Ù…Ø§ØªØ§ÙƒÙ„Ø´ Ø¯Ù…Ø§ØºÙŠ Ø£Ù†Ø§ Ù…Ø´ Ù†Ø§Ù‚Øµ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Ø§Ù„Ø¨Ù†Øª Ù„Ù…Ø§ ÙŠØ­ÙƒÙˆÙ„Ù‡Ø§ Ù„ÙŠÙ‡ Ø­Ø·Ù‡ Ù…ÙƒÙŠØ§Ø¬ ÙƒØªÙŠØ± ØªÙ‚ÙˆÙ„ Ø¯Ù‡ Ø¬...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>I'm just living off salads and yogurt and lovi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  sarcastic\n",
       "0    i want tech to be banned for use in work relat...          0\n",
       "1    happy birthday gorgeous! have an amazing day i...          0\n",
       "2    Thank you to   for organising pizza and ice cr...          1\n",
       "3    Ø§Ø­Ù„Ù‰ Ø£ØºÙ†ÙŠØ© ØªÙŠØªØ± ÙÙŠ Ø±Ù…Ø¶Ø§Ù† Ø§ØºÙ†ÙŠØ© Ø´ÙŠØ±ÙŠÙ† - Ù…Ø´Ø§Ø¹Ø± Ù…...          0\n",
       "4           Ø¨ÙŠØ´ØªØºÙ„ ØªÙ„Ø§Øª Ø³Ø§Ø¹Ø§Øª ÙˆÙŠØ±Ø¬Ø¹ ÙŠÙ‚ÙˆÙ„ ØªØ¹Ø¨Ø§Ù† Ù…Ø´ Ù‚Ø§Ø¯Ø±          1\n",
       "..                                                 ...        ...\n",
       "815  ÙˆØ­ÙŠØ§Øª Ø¹ÙŠÙ†ÙŠÙƒ ØŒØŒØŒØŒ Ø§Ø®Ø° ØµÙˆØª Ø§Ø®Ø° ÙˆØ¬Ù‡ Ø­Ø³Ù† ÙˆØ·Ø±Ø¨ Ø§ØµÙŠÙŠ...          0\n",
       "816  The most 2020 quote of the year: Me: we should...          1\n",
       "817                          Ù…Ø§ØªØ§ÙƒÙ„Ø´ Ø¯Ù…Ø§ØºÙŠ Ø£Ù†Ø§ Ù…Ø´ Ù†Ø§Ù‚Øµ          1\n",
       "818  Ø§Ù„Ø¨Ù†Øª Ù„Ù…Ø§ ÙŠØ­ÙƒÙˆÙ„Ù‡Ø§ Ù„ÙŠÙ‡ Ø­Ø·Ù‡ Ù…ÙƒÙŠØ§Ø¬ ÙƒØªÙŠØ± ØªÙ‚ÙˆÙ„ Ø¯Ù‡ Ø¬...          1\n",
       "819  I'm just living off salads and yogurt and lovi...          1\n",
       "\n",
       "[820 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = TweetDataset(\n",
    "        tweets=df.tweet.to_numpy(),\n",
    "        targets=df.sarcastic.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "train_data_loader = create_data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SarcasmClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.embed_size = self.bert.config.hidden_size\n",
    "        self.label_num = n_classes\n",
    "        self.fc_dropout = 0.2\n",
    "        self.hidden_num = 1\n",
    "        self.hidden_size = 256\n",
    "        self.hidden_dropout = 0.5\n",
    "        self.bidirectional = True\n",
    "        self.pad_size=32\n",
    "        \n",
    "\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.hidden_num,\n",
    "            bidirectional=True,\n",
    "            #dropout=self.hidden_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool1d(self.pad_size)\n",
    "\n",
    "        self.dropout=nn.Dropout(self.hidden_dropout)\n",
    "        self.fc_dropout = nn.Dropout(self.fc_dropout)\n",
    "        self.linear1 = nn.Linear(self.hidden_size *2, self.hidden_size//2)\n",
    "        self.linear2 = nn.Linear(self.hidden_size//2, self.label_num)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #sequence_output size: (batch_size, sequence_length, 768)\n",
    "        sequence_output, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        \n",
    "        lstm_out,(hidden_last,cn_last) = self.lstm(sequence_output)\n",
    "        #hidden = torch.cat((lstm_out[:,-1,:256], lstm_out[:,0,256:]), dim=-1)\n",
    "        #hidden = hidden.view(-1,256*2)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden_last_L= hidden_last[-2]\n",
    "            hidden_last_R = hidden_last[-1]\n",
    "            hidden = torch.cat([hidden_last_L, hidden_last_R],dim=-1)\n",
    "        else:\n",
    "            hidden=hidden_last[-1]\n",
    "        out=self.dropout(hidden)\n",
    "       # out = hidden.permute(0, 2, 1)\n",
    "       # out = self.maxpool(out).squeeze()\n",
    "        out = F.relu(self.linear1(hidden))\n",
    "        out = self.fc_dropout(out)\n",
    "        output = self.linear2(out)\n",
    "\n",
    "        return output\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmClassifier_Ori(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SarcasmClassifier_Ori, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SarcasmClassifier(len(class_names))\n",
    "#model = SarcasmClassifier_Ori(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight : torch.Size([119547, 768])\n",
      "bert.embeddings.position_embeddings.weight : torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight : torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight : torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.pooler.dense.weight : torch.Size([768, 768])\n",
      "bert.pooler.dense.bias : torch.Size([768])\n",
      "lstm.weight_ih_l0 : torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0 : torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 : torch.Size([1024])\n",
      "lstm.bias_hh_l0 : torch.Size([1024])\n",
      "lstm.weight_ih_l0_reverse : torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0_reverse : torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0_reverse : torch.Size([1024])\n",
      "lstm.bias_hh_l0_reverse : torch.Size([1024])\n",
      "linear1.weight : torch.Size([128, 512])\n",
      "linear1.bias : torch.Size([128])\n",
      "linear2.weight : torch.Size([2, 128])\n",
      "linear2.bias : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=4e-6, correct_bias=False)\n",
    "total_steps = len(train_data_loader)*EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, data_loader, loss_fn, optimizer, device, scheduler, n_examples\n",
    "):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        targets = d['targets'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss=loss_fn(outputs, targets)\n",
    "        correct_predictions +=torch.sum(preds==targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "   \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses=[]\n",
    "    correct_predictions=0\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids=d['input_ids'].to(device)\n",
    "            attention_mask=d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6522910053531329 accuracy 0.5896497647673811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.609310718683096 accuracy 0.6195121951219512\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5740719070037206 accuracy 0.683742812336644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6184410876952685 accuracy 0.6365853658536585\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4927321357031663 accuracy 0.7600627286983794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6590351685881615 accuracy 0.6463414634146342\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.42162517867982385 accuracy 0.8254051228437009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.69821120512027 accuracy 0.65\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3537476028005282 accuracy 0.8625196027182436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7489814253953787 accuracy 0.6475609756097561\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.2957061135520538 accuracy 0.8954521693674856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.8021491416371785 accuracy 0.6475609756097561\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.26329071695605916 accuracy 0.9121798222686879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.8292389781429217 accuracy 0.65\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.23890416255841654 accuracy 0.9221118661787767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.86642907674496 accuracy 0.6524390243902439\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54946/2144282225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{EPOCHS}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train loss {train_loss} accuracy {train_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54946/1079054722.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    print('-'*10)\n",
    "    train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    if val_acc>best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pinball!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So the Scottish Government want people to get ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>villainous pro tip : change the device name on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would date any of these men :pleading_face:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>Iâ€™ve just seen this and felt it deserved a Ret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>Omg how an earth is that a pen !! :clown_face:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>I love it when women are referred to as \"girl ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>The fact that people still don't get that you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     Size on the the Toulouse team, That pack is mo...          0\n",
       "1                                              Pinball!          0\n",
       "2     So the Scottish Government want people to get ...          1\n",
       "3     villainous pro tip : change the device name on...          0\n",
       "4         I would date any of these men :pleading_face:          0\n",
       "...                                                 ...        ...\n",
       "1395  Iâ€™ve just seen this and felt it deserved a Ret...          0\n",
       "1396     Omg how an earth is that a pen !! :clown_face:          0\n",
       "1397          Bringing Kanye and drake to a tl near you          0\n",
       "1398  I love it when women are referred to as \"girl ...          1\n",
       "1399  The fact that people still don't get that you ...          1\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_En_test = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/TaskA_Test.csv\")\n",
    "df_test = df_En_test.dropna(subset=['tweet','sarcastic'])\n",
    "df_test =df_test.dropna(axis=1,how='any')\n",
    "df_test =df_test.reset_index(drop=True)\n",
    "df_test['tweet']=df_test['tweet'].apply(clearTextFunc)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HL_test = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/Sarcasm_Headlines_Dataset_v2.csv\")\n",
    "df_HL_test = df_HL_test.drop('article_link',axis=1)\n",
    "df_HL_test = df_HL_test.rename(columns={'is_sarcastic':'sarcastic', 'headline':'tweet'})\n",
    "\n",
    "df_HL_test['tweet']=df_HL_test['tweet'].apply(clearTextFunc)\n",
    "df_test=df_HL_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader=create_data_loader(df_test, tokenizer,MAX_LEN,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5801740102728956"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc,_ = eval_model(\n",
    "    model, test_data_loader, loss_fn, device, len(df_test)\n",
    ")\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    tweet_texts=[]\n",
    "    predictions=[]\n",
    "    prediction_probs=[]\n",
    "    real_values=[]\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts=d['tweet_text']\n",
    "            input_ids=d['input_ids'].to(device)\n",
    "            attention_mask=d['attention_mask'].to(device)\n",
    "            targets=d['targets'].to(device)\n",
    "\n",
    "            outputs=model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds=torch.max(outputs,dim=1)\n",
    "            probs = F.softmax(outputs,dim=1)\n",
    "            tweet_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions=torch.stack(predictions).cpu()\n",
    "    prediction_probs=torch.stack(prediction_probs).cpu()\n",
    "    real_values=torch.stack(real_values).cpu()\n",
    "    return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(model, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.6738    0.6951    0.6843       410\n",
      "    sarcastic     0.6851    0.6634    0.6741       410\n",
      "\n",
      "     accuracy                         0.6793       820\n",
      "    macro avg     0.6794    0.6793    0.6792       820\n",
      " weighted avg     0.6794    0.6793    0.6792       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred,target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs,y_test = get_predictions(\n",
    "    model, test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.6063    0.5654    0.5851     14985\n",
      "    sarcastic     0.5553    0.5965    0.5751     13634\n",
      "\n",
      "     accuracy                         0.5802     28619\n",
      "    macro avg     0.5808    0.5809    0.5801     28619\n",
      " weighted avg     0.5820    0.5802    0.5804     28619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=class_names,digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f39ef18d35d925f5763d2a3e564ffd5568da5f87fa68eaddb65a9b6abdd95fcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
