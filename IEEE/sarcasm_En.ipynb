{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_names = ['not_sarcastic', 'sarcastic']\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集\n",
    "df_Cn = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/shuffledChineseSarcasmCorpus.csv\")\n",
    "df_En = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/train.En.csv\",usecols=['tweet','sarcastic'])\n",
    "df_Cn = df_Cn.rename(columns={'label':'sarcastic', 'text':'tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_En = df_En[:1734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1734 sarcastic 867 non sarcastic 867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_En.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "s_train = 0\n",
    "ns_train = 0\n",
    "for i in df_En.sarcastic:\n",
    "    if i == 1:\n",
    "        s_train+=1\n",
    "    else:\n",
    "        ns_train+=1\n",
    "l_count = len(df_En.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", s_train, \"non sarcastic\", ns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>大国重器蓝色力量！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>东野圭吾！你的白夜行可以再灰色一点没关系... \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>机不可失，失不再来，符合条件的朋友们，冲鸭！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1</td>\n",
       "      <td>马克龙虽出生在法国，但既然姓马，就可以得知他的祖先是中国人。而既然是中国人，名字却叫克龙，他...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>正在直播：失事飞机第二部黑匣子已找到 报社记者直击搜寻现场。记者27日从“3·21”东航MU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1</td>\n",
       "      <td>【谁说传销一无是处？年轻人入传销组织或可根治社恐症】 ​​​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>1个简单动作缓解腰部酸痛。久坐或长时间劳动后有肌肉僵硬、酸痛的感觉？试试这个动作，帮你缓解腰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1</td>\n",
       "      <td>恶心，驱长说礼物退回人家会伤心后，又改口收的礼物已经送给弱势！（你可以再ㄍㄟˋ一点） \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sarcastic                                              tweet\n",
       "0             0                                          大国重器蓝色力量！\n",
       "1             1                         东野圭吾！你的白夜行可以再灰色一点没关系... \\n\n",
       "2             0                             机不可失，失不再来，符合条件的朋友们，冲鸭！\n",
       "3             1                    我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n\n",
       "4             1  近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...\n",
       "...         ...                                                ...\n",
       "1995          1  马克龙虽出生在法国，但既然姓马，就可以得知他的祖先是中国人。而既然是中国人，名字却叫克龙，他...\n",
       "1996          0  正在直播：失事飞机第二部黑匣子已找到 报社记者直击搜寻现场。记者27日从“3·21”东航MU...\n",
       "1997          1                     【谁说传销一无是处？年轻人入传销组织或可根治社恐症】 ​​​\n",
       "1998          0  1个简单动作缓解腰部酸痛。久坐或长时间劳动后有肌肉僵硬、酸痛的感觉？试试这个动作，帮你缓解腰...\n",
       "1999          1      恶心，驱长说礼物退回人家会伤心后，又改口收的礼物已经送给弱势！（你可以再ㄍㄟˋ一点） \\n\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>东野圭吾！你的白夜行可以再灰色一点没关系... \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>火大：你的声音真的可以再大声一点，可以再故意用力一点。 真的是会被你的声音活活给吓死~\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>昨日因吸毒被抓的几位迷笛音乐学校的学生面对媒体时吐露，他们是因为偶像尹相杰才走上音乐的道路，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>学校里的安全教育还是很有效的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sarcastic                                              tweet\n",
       "0            1                         东野圭吾！你的白夜行可以再灰色一点没关系... \\n\n",
       "1            1                    我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n\n",
       "2            1  近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...\n",
       "3            1      火大：你的声音真的可以再大声一点，可以再故意用力一点。 真的是会被你的声音活活给吓死~\\n\n",
       "4            1  昨日因吸毒被抓的几位迷笛音乐学校的学生面对媒体时吐露，他们是因为偶像尹相杰才走上音乐的道路，...\n",
       "..         ...                                                ...\n",
       "995          0                                    学校里的安全教育还是很有效的。\n",
       "996          0  俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...\n",
       "997          0  数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...\n",
       "998          0  北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...\n",
       "999          0  上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Cn_sar=df_Cn.loc[df_Cn['sarcastic'] == 1]\n",
    "df_Cn_nsar=df_Cn.loc[df_Cn['sarcastic'] == 0]\n",
    "df_Cn = pd.concat((df_Cn_sar[:500], df_Cn_nsar[:500]), join='outer')\n",
    "df_Cn =df_Cn.reset_index(drop=True)\n",
    "df_Cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1000 sarcastic 500 non sarcastic 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = sns.countplot(df_Cn.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "cs_train = 0\n",
    "ncs_train = 0\n",
    "for i in df_Cn.sarcastic:\n",
    "    if i == 1:\n",
    "        cs_train+=1\n",
    "    else:\n",
    "        ncs_train+=1\n",
    "l_count = len(df_Cn.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", cs_train, \"non sarcastic\", ncs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet, add_special_tokens=True, max_length=self.max_len, \n",
    "            return_token_type_ids=False, pad_to_max_length=True,\n",
    "            return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'tweet_text':tweet,\n",
    "            'input_ids':encoding['input_ids'].flatten(),\n",
    "            'attention_mask':encoding['attention_mask'].flatten(),\n",
    "            'targets':torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>i just overheard this bunch of children behind...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>started telling my dutch friends about how i'v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>took my dog to the vet today because i thought...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>have to somehow become fluent in Spanish in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>complained about my job for weeks but i just c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "1729  i just overheard this bunch of children behind...          0\n",
       "1730  started telling my dutch friends about how i'v...          0\n",
       "1731  took my dog to the vet today because i thought...          0\n",
       "1732  have to somehow become fluent in Spanish in th...          0\n",
       "1733  complained about my job for weeks but i just c...          0\n",
       "\n",
       "[1734 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>i just overheard this bunch of children behind...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>started telling my dutch friends about how i'v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>took my dog to the vet today because i thought...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>have to somehow become fluent in Spanish in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>complained about my job for weeks but i just c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1733 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "1728  i just overheard this bunch of children behind...          0\n",
       "1729  started telling my dutch friends about how i'v...          0\n",
       "1730  took my dog to the vet today because i thought...          0\n",
       "1731  have to somehow become fluent in Spanish in th...          0\n",
       "1732  complained about my job for weeks but i just c...          0\n",
       "\n",
       "[1733 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_total=df_En\n",
    "df_total = df_total.dropna(subset=['tweet'])\n",
    "df_total =df_total.dropna(axis=1,how='any')\n",
    "df_total =df_total.reset_index(drop=True)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clearTextFunc(text):\n",
    "    clear_text= \" \".join(text.split())\n",
    "    clear_text=re.sub(r'http://[a-zA-Z0-9.?/&=:]*',\" \", clear_text) #去除URL\n",
    "    clear_text=re.sub(r'https://[a-zA-Z0-9.?/&=:]*',\" \", clear_text) #去除URL\n",
    "    clear_text=re.sub(r'@[a-zA-Z0-9.?/&=:]*',\" \", clear_text)#去除@\n",
    "    clear_text=re.sub(r'/s',\" \",clear_text)\n",
    "    clear_text=re.sub(r'/j',\" \",clear_text)\n",
    "\n",
    "    return clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I did too, and I also reported Cancun Cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>i just overheard this bunch of children behind...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>started telling my dutch friends about how i'v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>took my dog to the vet today because i thought...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>have to somehow become fluent in Spanish in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>complained about my job for weeks but i just c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1733 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4           I did too, and I also reported Cancun Cr...          1\n",
       "...                                                 ...        ...\n",
       "1728  i just overheard this bunch of children behind...          0\n",
       "1729  started telling my dutch friends about how i'v...          0\n",
       "1730  took my dog to the vet today because i thought...          0\n",
       "1731  have to somehow become fluent in Spanish in th...          0\n",
       "1732  complained about my job for weeks but i just c...          0\n",
       "\n",
       "[1733 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total['tweet']=df_total['tweet'].apply(clearTextFunc)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1733 sarcastic 867 non sarcastic 866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_total.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "ts_train = 0\n",
    "tns_train = 0\n",
    "for i in df_total.sarcastic:\n",
    "    if i == 1:\n",
    "        ts_train+=1\n",
    "    else:\n",
    "        tns_train+=1\n",
    "l_count = len(df_total.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", ts_train, \"non sarcastic\", tns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=RANDOM_SEED)\n",
    "for train_index, text_index in split.split(df_total, df_total['sarcastic']):\n",
    "    df_train = df_total.loc[train_index]\n",
    "    df_val = df_total.loc[text_index]\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everyone say a prayer to the insurance gods th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Having my first experience of a Whetherspoon h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who likes cats?? These handmade cushions are p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well done Henri, cost us the win #nffc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have to somehow become fluent in Spanish in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>Join me on   to help save our planet in just 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>Wishing all the birthday sex on   today 🎂</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>Beautician and the Beast (1997). Best Timothy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>cold weather please hurry i want to wear sweat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>Men love making women think they were the prob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     everyone say a prayer to the insurance gods th...          1\n",
       "1     Having my first experience of a Whetherspoon h...          0\n",
       "2     Who likes cats?? These handmade cushions are p...          0\n",
       "3                Well done Henri, cost us the win #nffc          1\n",
       "4     have to somehow become fluent in Spanish in th...          0\n",
       "...                                                 ...        ...\n",
       "1208  Join me on   to help save our planet in just 2...          0\n",
       "1209        Wishing all the birthday sex on   today 🎂            1\n",
       "1210  Beautician and the Beast (1997). Best Timothy ...          1\n",
       "1211  cold weather please hurry i want to wear sweat...          0\n",
       "1212  Men love making women think they were the prob...          1\n",
       "\n",
       "[1213 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Totall Carloss... #WTF1  _aarava</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think the episode really fails cos of th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am honestly so disturbed by the star signs cha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rom coms from the 2000s are simply built diffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Las Iguanas 👌😍🍷🍴</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>I love how men know they have a baby on the wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>I’m going back to work exactly 8 weeks postpar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Let me guess, it has to be Charlotte Tilbury m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Fun fact: I sing I Morgan ar en annan dag to m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>I claimed spy. Was I? I was a spy the whole ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  sarcastic\n",
       "0                   Totall Carloss... #WTF1  _aarava            0\n",
       "1        I think the episode really fails cos of th...          0\n",
       "2    am honestly so disturbed by the star signs cha...          0\n",
       "3    rom coms from the 2000s are simply built diffe...          0\n",
       "4                                     Las Iguanas 👌😍🍷🍴          0\n",
       "..                                                 ...        ...\n",
       "515  I love how men know they have a baby on the wa...          1\n",
       "516  I’m going back to work exactly 8 weeks postpar...          0\n",
       "517  Let me guess, it has to be Charlotte Tilbury m...          1\n",
       "518  Fun fact: I sing I Morgan ar en annan dag to m...          0\n",
       "519  I claimed spy. Was I? I was a spy the whole ti...          0\n",
       "\n",
       "[520 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = TweetDataset(\n",
    "        tweets=df.tweet.to_numpy(),\n",
    "        targets=df.sarcastic.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "train_data_loader = create_data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SarcasmClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.embed_size = self.bert.config.hidden_size\n",
    "        self.label_num = n_classes\n",
    "        self.fc_dropout = 0.2\n",
    "        self.hidden_num = 1\n",
    "        self.hidden_size = 256\n",
    "        self.hidden_dropout = 0.5\n",
    "        self.bidirectional = True\n",
    "        self.pad_size=32\n",
    "        \n",
    "\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.hidden_num,\n",
    "            bidirectional=True,\n",
    "            #dropout=self.hidden_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool1d(self.pad_size)\n",
    "\n",
    "        self.dropout=nn.Dropout(self.hidden_dropout)\n",
    "        self.fc_dropout = nn.Dropout(self.fc_dropout)\n",
    "        self.linear1 = nn.Linear(self.hidden_size *2, self.hidden_size//2)\n",
    "        self.linear2 = nn.Linear(self.hidden_size//2, self.label_num)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #sequence_output size: (batch_size, sequence_length, 768)\n",
    "        sequence_output, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        \n",
    "        lstm_out,(hidden_last,cn_last) = self.lstm(sequence_output)\n",
    "        #hidden = torch.cat((lstm_out[:,-1,:256], lstm_out[:,0,256:]), dim=-1)\n",
    "        #hidden = hidden.view(-1,256*2)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden_last_L= hidden_last[-2]\n",
    "            hidden_last_R = hidden_last[-1]\n",
    "            hidden = torch.cat([hidden_last_L, hidden_last_R],dim=-1)\n",
    "        else:\n",
    "            hidden=hidden_last[-1]\n",
    "        out=self.dropout(hidden)\n",
    "       # out = hidden.permute(0, 2, 1)\n",
    "       # out = self.maxpool(out).squeeze()\n",
    "        out = F.relu(self.linear1(hidden))\n",
    "        out = self.fc_dropout(out)\n",
    "        output = self.linear2(out)\n",
    "\n",
    "        return output\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmClassifier_Ori(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SarcasmClassifier_Ori, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SarcasmClassifier(len(class_names))\n",
    "#model = SarcasmClassifier_Ori(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight : torch.Size([119547, 768])\n",
      "bert.embeddings.position_embeddings.weight : torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight : torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight : torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.pooler.dense.weight : torch.Size([768, 768])\n",
      "bert.pooler.dense.bias : torch.Size([768])\n",
      "lstm.weight_ih_l0 : torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0 : torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 : torch.Size([1024])\n",
      "lstm.bias_hh_l0 : torch.Size([1024])\n",
      "lstm.weight_ih_l0_reverse : torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0_reverse : torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0_reverse : torch.Size([1024])\n",
      "lstm.bias_hh_l0_reverse : torch.Size([1024])\n",
      "linear1.weight : torch.Size([128, 512])\n",
      "linear1.bias : torch.Size([128])\n",
      "linear2.weight : torch.Size([2, 128])\n",
      "linear2.bias : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=4e-6, correct_bias=False)\n",
    "total_steps = len(train_data_loader)*EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, data_loader, loss_fn, optimizer, device, scheduler, n_examples\n",
    "):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        targets = d['targets'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss=loss_fn(outputs, targets)\n",
    "        correct_predictions +=torch.sum(preds==targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "   \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses=[]\n",
    "    correct_predictions=0\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids=d['input_ids'].to(device)\n",
    "            attention_mask=d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6940916862926985 accuracy 0.494641384995878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6940819729458202 accuracy 0.5\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6882078490759197 accuracy 0.5457543281121188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6930876916105096 accuracy 0.5076923076923077\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.652655207012829 accuracy 0.651277823577906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6844371105685378 accuracy 0.5673076923076923\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5797409815223593 accuracy 0.7452596867271228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6905917218237212 accuracy 0.5942307692307692\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5055749251654273 accuracy 0.8145094806265458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7090212034456658 accuracy 0.5903846153846154\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.438326091750672 accuracy 0.8499587798845837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7343038157983259 accuracy 0.5865384615384616\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3899988471285293 accuracy 0.8812860676009893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7630673430182717 accuracy 0.5826923076923077\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.356558465447865 accuracy 0.8936521022258862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7908184284513647 accuracy 0.5730769230769232\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3282697865445363 accuracy 0.9150865622423743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.8157469512838306 accuracy 0.5692307692307692\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.30883484294540003 accuracy 0.9241549876339654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.8348478223338271 accuracy 0.5750000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    print('-'*10)\n",
    "    train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    if val_acc>best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pinball!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So the Scottish Government want people to get ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>villainous pro tip : change the device name on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would date any of these men :pleading_face:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>Omg how an earth is that a pen !! :clown_face:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>I love it when women are referred to as \"girl ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>The fact that people still don't get that you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     Size on the the Toulouse team, That pack is mo...          0\n",
       "1                                              Pinball!          0\n",
       "2     So the Scottish Government want people to get ...          1\n",
       "3     villainous pro tip : change the device name on...          0\n",
       "4         I would date any of these men :pleading_face:          0\n",
       "...                                                 ...        ...\n",
       "1395  I’ve just seen this and felt it deserved a Ret...          0\n",
       "1396     Omg how an earth is that a pen !! :clown_face:          0\n",
       "1397          Bringing Kanye and drake to a tl near you          0\n",
       "1398  I love it when women are referred to as \"girl ...          1\n",
       "1399  The fact that people still don't get that you ...          1\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_En_test = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/TaskA_Test.csv\")\n",
    "df_test = df_En_test.dropna(subset=['tweet','sarcastic'])\n",
    "df_test =df_test.dropna(axis=1,how='any')\n",
    "df_test =df_test.reset_index(drop=True)\n",
    "df_test['tweet']=df_test['tweet'].apply(clearTextFunc)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HL_test = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/Sarcasm_Headlines_Dataset_v2.csv\")\n",
    "df_HL_test = df_HL_test.drop('article_link',axis=1)\n",
    "df_HL_test = df_HL_test.rename(columns={'is_sarcastic':'sarcastic', 'headline':'tweet'})\n",
    "\n",
    "df_HL_test['tweet']=df_HL_test['tweet'].apply(clearTextFunc)\n",
    "df_test=df_HL_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader=create_data_loader(df_test, tokenizer,MAX_LEN,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44357142857142856"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc,_ = eval_model(\n",
    "    model, test_data_loader, loss_fn, device, len(df_test)\n",
    ")\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    tweet_texts=[]\n",
    "    predictions=[]\n",
    "    prediction_probs=[]\n",
    "    real_values=[]\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts=d['tweet_text']\n",
    "            input_ids=d['input_ids'].to(device)\n",
    "            attention_mask=d['attention_mask'].to(device)\n",
    "            targets=d['targets'].to(device)\n",
    "\n",
    "            outputs=model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds=torch.max(outputs,dim=1)\n",
    "            probs = F.softmax(outputs,dim=1)\n",
    "            tweet_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions=torch.stack(predictions).cpu()\n",
    "    prediction_probs=torch.stack(prediction_probs).cpu()\n",
    "    real_values=torch.stack(real_values).cpu()\n",
    "    return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(model, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.5980    0.4577    0.5185       260\n",
      "    sarcastic     0.5607    0.6923    0.6196       260\n",
      "\n",
      "     accuracy                         0.5750       520\n",
      "    macro avg     0.5794    0.5750    0.5691       520\n",
      " weighted avg     0.5794    0.5750    0.5691       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred,target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs,y_test = get_predictions(\n",
    "    model, test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.8920    0.3992    0.5515      1200\n",
      "    sarcastic     0.1645    0.7100    0.2672       200\n",
      "\n",
      "     accuracy                         0.4436      1400\n",
      "    macro avg     0.5283    0.5546    0.4093      1400\n",
      " weighted avg     0.7881    0.4436    0.5109      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=class_names,digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f39ef18d35d925f5763d2a3e564ffd5568da5f87fa68eaddb65a9b6abdd95fcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
