{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_names = ['not_sarcastic', 'sarcastic']\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集\n",
    "df_Cn = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/shuffledChineseSarcasmCorpus.csv\")\n",
    "df_En = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/train.En.csv\")\n",
    "df_Cn = df_Cn.rename(columns={'label':'sarcastic', 'text':'tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_En = df_En[:1734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1734 sarcastic 867 non sarcastic 867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_En.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "s_train = 0\n",
    "ns_train = 0\n",
    "for i in df_En.sarcastic:\n",
    "    if i == 1:\n",
    "        s_train+=1\n",
    "    else:\n",
    "        ns_train+=1\n",
    "l_count = len(df_En.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", s_train, \"non sarcastic\", ns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>大国重器蓝色力量！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>东野圭吾！你的白夜行可以再灰色一点没关系... \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>机不可失，失不再来，符合条件的朋友们，冲鸭！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1</td>\n",
       "      <td>马克龙虽出生在法国，但既然姓马，就可以得知他的祖先是中国人。而既然是中国人，名字却叫克龙，他...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>正在直播：失事飞机第二部黑匣子已找到 报社记者直击搜寻现场。记者27日从“3·21”东航MU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1</td>\n",
       "      <td>【谁说传销一无是处？年轻人入传销组织或可根治社恐症】 ​​​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>1个简单动作缓解腰部酸痛。久坐或长时间劳动后有肌肉僵硬、酸痛的感觉？试试这个动作，帮你缓解腰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1</td>\n",
       "      <td>恶心，驱长说礼物退回人家会伤心后，又改口收的礼物已经送给弱势！（你可以再ㄍㄟˋ一点） \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sarcastic                                              tweet\n",
       "0             0                                          大国重器蓝色力量！\n",
       "1             1                         东野圭吾！你的白夜行可以再灰色一点没关系... \\n\n",
       "2             0                             机不可失，失不再来，符合条件的朋友们，冲鸭！\n",
       "3             1                    我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n\n",
       "4             1  近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...\n",
       "...         ...                                                ...\n",
       "1995          1  马克龙虽出生在法国，但既然姓马，就可以得知他的祖先是中国人。而既然是中国人，名字却叫克龙，他...\n",
       "1996          0  正在直播：失事飞机第二部黑匣子已找到 报社记者直击搜寻现场。记者27日从“3·21”东航MU...\n",
       "1997          1                     【谁说传销一无是处？年轻人入传销组织或可根治社恐症】 ​​​\n",
       "1998          0  1个简单动作缓解腰部酸痛。久坐或长时间劳动后有肌肉僵硬、酸痛的感觉？试试这个动作，帮你缓解腰...\n",
       "1999          1      恶心，驱长说礼物退回人家会伤心后，又改口收的礼物已经送给弱势！（你可以再ㄍㄟˋ一点） \\n\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>东野圭吾！你的白夜行可以再灰色一点没关系... \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>火大：你的声音真的可以再大声一点，可以再故意用力一点。 真的是会被你的声音活活给吓死~\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>昨日因吸毒被抓的几位迷笛音乐学校的学生面对媒体时吐露，他们是因为偶像尹相杰才走上音乐的道路，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>学校里的安全教育还是很有效的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sarcastic                                              tweet\n",
       "0            1                         东野圭吾！你的白夜行可以再灰色一点没关系... \\n\n",
       "1            1                    我妈最常说的一句话：你脾气那么坏，谁敢娶你...(很好 )\\n\n",
       "2            1  近日，几十位资深专家经过连续多日反复论证研究得出观点：那些把共享单车又是投河又是破坏的人情有...\n",
       "3            1      火大：你的声音真的可以再大声一点，可以再故意用力一点。 真的是会被你的声音活活给吓死~\\n\n",
       "4            1  昨日因吸毒被抓的几位迷笛音乐学校的学生面对媒体时吐露，他们是因为偶像尹相杰才走上音乐的道路，...\n",
       "..         ...                                                ...\n",
       "995          0                                    学校里的安全教育还是很有效的。\n",
       "996          0  俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...\n",
       "997          0  数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...\n",
       "998          0  北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...\n",
       "999          0  上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Cn_sar=df_Cn.loc[df_Cn['sarcastic'] == 1]\n",
    "df_Cn_nsar=df_Cn.loc[df_Cn['sarcastic'] == 0]\n",
    "df_Cn = pd.concat((df_Cn_sar[:500], df_Cn_nsar[:500]), join='outer')\n",
    "df_Cn =df_Cn.reset_index(drop=True)\n",
    "df_Cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1000 sarcastic 500 non sarcastic 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = sns.countplot(df_Cn.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "cs_train = 0\n",
    "ncs_train = 0\n",
    "for i in df_Cn.sarcastic:\n",
    "    if i == 1:\n",
    "        cs_train+=1\n",
    "    else:\n",
    "        ncs_train+=1\n",
    "l_count = len(df_Cn.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", cs_train, \"non sarcastic\", ncs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet, add_special_tokens=True, max_length=self.max_len, \n",
    "            return_token_type_ids=False, pad_to_max_length=True,\n",
    "            return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'tweet_text':tweet,\n",
    "            'input_ids':encoding['input_ids'].flatten(),\n",
    "            'attention_mask':encoding['attention_mask'].flatten(),\n",
    "            'targets':torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I do not like when professors don’t write out ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1729</td>\n",
       "      <td>i just overheard this bunch of children behind...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>1730</td>\n",
       "      <td>started telling my dutch friends about how i'v...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1731</td>\n",
       "      <td>took my dog to the vet today because i thought...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>1732</td>\n",
       "      <td>have to somehow become fluent in Spanish in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1733</td>\n",
       "      <td>complained about my job for weeks but i just c...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  The only thing I got from college is a caffein...   \n",
       "1              1  I love it when professors draw a big question ...   \n",
       "2              2  Remember the hundred emails from companies whe...   \n",
       "3              3  Today my pop-pop told me I was not “forced” to...   \n",
       "4              4  @VolphanCarol @littlewhitty @mysticalmanatee I...   \n",
       "...          ...                                                ...   \n",
       "1729        1729  i just overheard this bunch of children behind...   \n",
       "1730        1730  started telling my dutch friends about how i'v...   \n",
       "1731        1731  took my dog to the vet today because i thought...   \n",
       "1732        1732  have to somehow become fluent in Spanish in th...   \n",
       "1733        1733  complained about my job for weeks but i just c...   \n",
       "\n",
       "      sarcastic                                           rephrase  sarcasm  \\\n",
       "0             1  College is really difficult, expensive, tiring...      0.0   \n",
       "1             1  I do not like when professors don’t write out ...      1.0   \n",
       "2             1  I, at the bare minimum, wish companies actuall...      0.0   \n",
       "3             1  Today my pop-pop told me I was not \"forced\" to...      1.0   \n",
       "4             1  I would say Ted Cruz is an asshole and doesn’t...      1.0   \n",
       "...         ...                                                ...      ...   \n",
       "1729          0                                                NaN      NaN   \n",
       "1730          0                                                NaN      NaN   \n",
       "1731          0                                                NaN      NaN   \n",
       "1732          0                                                NaN      NaN   \n",
       "1733          0                                                NaN      NaN   \n",
       "\n",
       "      irony  satire  understatement  overstatement  rhetorical_question  \n",
       "0       1.0     0.0             0.0            0.0                  0.0  \n",
       "1       0.0     0.0             0.0            0.0                  0.0  \n",
       "2       1.0     0.0             0.0            0.0                  0.0  \n",
       "3       0.0     0.0             0.0            0.0                  0.0  \n",
       "4       0.0     0.0             0.0            0.0                  0.0  \n",
       "...     ...     ...             ...            ...                  ...  \n",
       "1729    NaN     NaN             NaN            NaN                  NaN  \n",
       "1730    NaN     NaN             NaN            NaN                  NaN  \n",
       "1731    NaN     NaN             NaN            NaN                  NaN  \n",
       "1732    NaN     NaN             NaN            NaN                  NaN  \n",
       "1733    NaN     NaN             NaN            NaN                  NaN  \n",
       "\n",
       "[1734 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>学校里的安全教育还是很有效的。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2733 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "2728                                    学校里的安全教育还是很有效的。          0\n",
       "2729  俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...          0\n",
       "2730  数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...          0\n",
       "2731  北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...          0\n",
       "2732  上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...          0\n",
       "\n",
       "[2733 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = pd.concat((df_En, df_Cn), join='outer')\n",
    "\n",
    "df_total = df_total.dropna(subset=['tweet'])\n",
    "df_total =df_total.dropna(axis=1,how='any')\n",
    "df_total =df_total.reset_index(drop=True)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clearTextFunc(text):\n",
    "    clear_text= \" \".join(text.split())\n",
    "    clear_text=re.sub(r'http://[a-zA-Z0-9.?/&=:]*',\" \", clear_text) #去除URL\n",
    "    clear_text=re.sub(r'https://[a-zA-Z0-9.?/&=:]*',\" \", clear_text) #去除URL\n",
    "    clear_text=re.sub(r'@[a-zA-Z0-9.?/&=:]*',\" \", clear_text)#去除@\n",
    "    clear_text=re.sub(r'/s',\" \",clear_text)\n",
    "    clear_text=re.sub(r'/j',\" \",clear_text)\n",
    "\n",
    "    return clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I did too, and I also reported Cancun Cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>学校里的安全教育还是很有效的。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2733 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4           I did too, and I also reported Cancun Cr...          1\n",
       "...                                                 ...        ...\n",
       "2728                                    学校里的安全教育还是很有效的。          0\n",
       "2729  俄罗斯超伊朗成被美制裁最多国家，赵立坚称战争和制裁让美国赚得钵满盆满。4月6日，外交部例行记...          0\n",
       "2730  数据网站：2022年前三月美国逾万人死于枪支暴力。美国非营利组织“枪支暴力档案”网站30日最...          0\n",
       "2731  北京警方查控逃避防疫检查进京人员223人。记者26日从北京市公安局了解到，北京警方近期严打逃...          0\n",
       "2732  上海两区首日达到社会面清零目标。20日，上海市在新闻发布会上通报，上海全市疫情近几天呈下降趋...          0\n",
       "\n",
       "[2733 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total['tweet']=df_total['tweet'].apply(clearTextFunc)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 2733 sarcastic 1367 non sarcastic 1366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_total.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "ts_train = 0\n",
    "tns_train = 0\n",
    "for i in df_total.sarcastic:\n",
    "    if i == 1:\n",
    "        ts_train+=1\n",
    "    else:\n",
    "        tns_train+=1\n",
    "l_count = len(df_total.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", ts_train, \"non sarcastic\", tns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=RANDOM_SEED)\n",
    "for train_index, text_index in split.split(df_total, df_total['sarcastic']):\n",
    "    df_train = df_total.loc[train_index]\n",
    "    df_val = df_total.loc[text_index]\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_Warner     Not sure if this has been answe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This second jab has made me feel off the box t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My dad is bringing me home Spider-Man Vans fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当美国的敌人是危险的，但当美国的盟友是致命的 辛格基</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Snowden Sad to see intelligent Americans fall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>挤干水分会很难看的 马科长会越来越多的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>I’m at a weird point in my life where I’m on e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>.....没关系...很好...你就继续吧....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>共用影印的电脑 可以再慢一点阿~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>If they aren’t out yet just wait to get the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1913 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0        _Warner     Not sure if this has been answe...          0\n",
       "1     This second jab has made me feel off the box t...          0\n",
       "2     My dad is bringing me home Spider-Man Vans fro...          0\n",
       "3                            当美国的敌人是危险的，但当美国的盟友是致命的 辛格基          0\n",
       "4     #Snowden Sad to see intelligent Americans fall...          0\n",
       "...                                                 ...        ...\n",
       "1908                                挤干水分会很难看的 马科长会越来越多的          0\n",
       "1909  I’m at a weird point in my life where I’m on e...          0\n",
       "1910                          .....没关系...很好...你就继续吧....          1\n",
       "1911                                   共用影印的电脑 可以再慢一点阿~          1\n",
       "1912    If they aren’t out yet just wait to get the ...          0\n",
       "\n",
       "[1913 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i want tech to be banned for use in work relat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy birthday gorgeous! have an amazing day i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you to   for organising pizza and ice cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>清明奇妙游。河南卫视神仙节目上新了。清明是一缕思念，满怀牵挂；清明是一份诗意，充满希望。春和...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2334 unit 要RMA 也太幸运了吧~ 第一次RMA的数量那么惊人</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>1个动作缓解腰酸背痛。日常生活中经常会因为久坐等，出现肌肉酸痛、僵硬，一套“脊柱舒展运动”转...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>The most 2020 quote of the year: Me: we should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>俄罗斯反同新法案起草人议员尼基查克(Ivan Nikitchuk)表示，反同新法案将再出重拳...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>cow...你的字可以再丑一点..写什么鬼东东阿！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>I'm just living off salads and yogurt and lovi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  sarcastic\n",
       "0    i want tech to be banned for use in work relat...          0\n",
       "1    happy birthday gorgeous! have an amazing day i...          0\n",
       "2    Thank you to   for organising pizza and ice cr...          1\n",
       "3    清明奇妙游。河南卫视神仙节目上新了。清明是一缕思念，满怀牵挂；清明是一份诗意，充满希望。春和...          0\n",
       "4                 2334 unit 要RMA 也太幸运了吧~ 第一次RMA的数量那么惊人          1\n",
       "..                                                 ...        ...\n",
       "815  1个动作缓解腰酸背痛。日常生活中经常会因为久坐等，出现肌肉酸痛、僵硬，一套“脊柱舒展运动”转...          0\n",
       "816  The most 2020 quote of the year: Me: we should...          1\n",
       "817  俄罗斯反同新法案起草人议员尼基查克(Ivan Nikitchuk)表示，反同新法案将再出重拳...          1\n",
       "818                          cow...你的字可以再丑一点..写什么鬼东东阿！          1\n",
       "819  I'm just living off salads and yogurt and lovi...          1\n",
       "\n",
       "[820 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = TweetDataset(\n",
    "        tweets=df.tweet.to_numpy(),\n",
    "        targets=df.sarcastic.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "train_data_loader = create_data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SarcasmClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.embed_size = self.bert.config.hidden_size\n",
    "        self.label_num = n_classes\n",
    "        self.fc_dropout = 0.2\n",
    "        self.hidden_num = 1\n",
    "        self.hidden_size = 256\n",
    "        self.hidden_dropout = 0.5\n",
    "        self.bidirectional = True\n",
    "        self.pad_size=32\n",
    "        \n",
    "\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.hidden_num,\n",
    "            bidirectional=True,\n",
    "            #dropout=self.hidden_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool1d(self.pad_size)\n",
    "\n",
    "        self.dropout=nn.Dropout(self.hidden_dropout)\n",
    "        self.fc_dropout = nn.Dropout(self.fc_dropout)\n",
    "        self.linear1 = nn.Linear(self.hidden_size *2, self.hidden_size//2)\n",
    "        self.linear2 = nn.Linear(self.hidden_size//2, self.label_num)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #sequence_output size: (batch_size, sequence_length, 768)\n",
    "        sequence_output, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        \n",
    "        lstm_out,(hidden_last,cn_last) = self.lstm(sequence_output)\n",
    "        #hidden = torch.cat((lstm_out[:,-1,:256], lstm_out[:,0,256:]), dim=-1)\n",
    "        #hidden = hidden.view(-1,256*2)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden_last_L= hidden_last[-2]\n",
    "            hidden_last_R = hidden_last[-1]\n",
    "            hidden = torch.cat([hidden_last_L, hidden_last_R],dim=-1)\n",
    "        else:\n",
    "            hidden=hidden_last[-1]\n",
    "        out=self.dropout(hidden)\n",
    "       # out = hidden.permute(0, 2, 1)\n",
    "       # out = self.maxpool(out).squeeze()\n",
    "        out = F.relu(self.linear1(hidden))\n",
    "        out = self.fc_dropout(out)\n",
    "        output = self.linear2(out)\n",
    "\n",
    "        return output\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmClassifier_Ori(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SarcasmClassifier_Ori, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SarcasmClassifier(len(class_names))\n",
    "#model = SarcasmClassifier_Ori(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight : torch.Size([119547, 768])\n",
      "bert.embeddings.position_embeddings.weight : torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight : torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight : torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight : torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias : torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight : torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias : torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight : torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias : torch.Size([768])\n",
      "bert.pooler.dense.weight : torch.Size([768, 768])\n",
      "bert.pooler.dense.bias : torch.Size([768])\n",
      "lstm.weight_ih_l0 : torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0 : torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 : torch.Size([1024])\n",
      "lstm.bias_hh_l0 : torch.Size([1024])\n",
      "lstm.weight_ih_l0_reverse : torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0_reverse : torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0_reverse : torch.Size([1024])\n",
      "lstm.bias_hh_l0_reverse : torch.Size([1024])\n",
      "linear1.weight : torch.Size([128, 512])\n",
      "linear1.bias : torch.Size([128])\n",
      "linear2.weight : torch.Size([2, 128])\n",
      "linear2.bias : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=4e-6, correct_bias=False)\n",
    "total_steps = len(train_data_loader)*EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, data_loader, loss_fn, optimizer, device, scheduler, n_examples\n",
    "):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        targets = d['targets'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss=loss_fn(outputs, targets)\n",
    "        correct_predictions +=torch.sum(preds==targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "   \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses=[]\n",
    "    correct_predictions=0\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids=d['input_ids'].to(device)\n",
    "            attention_mask=d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6457073852419853 accuracy 0.6131730266596968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.5697316785271351 accuracy 0.6646341463414634\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.522728867828846 accuracy 0.7375849451123889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.5465112225367472 accuracy 0.6865853658536586\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4260806639989217 accuracy 0.8165185572399373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.5750322261681924 accuracy 0.7060975609756097\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.35430516774455706 accuracy 0.8593831677992682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.6173453892652805 accuracy 0.6939024390243902\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.2848151652763287 accuracy 0.8996340825927862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.655580647289753 accuracy 0.7060975609756097\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.2212878079774479 accuracy 0.9309984317825405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7378626729433353 accuracy 0.6890243902439024\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.19879721167186895 accuracy 0.9393622582331417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.7818806638511328 accuracy 0.7085365853658536\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.16873513634006182 accuracy 0.9498170412963931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.8058730828074309 accuracy 0.7085365853658536\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.1569380447578927 accuracy 0.9576581285938316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.825578894179601 accuracy 0.7048780487804878\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.15539076505228877 accuracy 0.9550444328280188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.847673048480199 accuracy 0.7073170731707317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    print('-'*10)\n",
    "    train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    if val_acc>best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pinball!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So the Scottish Government want people to get ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>villainous pro tip : change the device name on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I would date any of these men :pleading_face:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>Omg how an earth is that a pen !! :clown_face:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>I love it when women are referred to as \"girl ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>The fact that people still don't get that you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     Size on the the Toulouse team, That pack is mo...          0\n",
       "1                                              Pinball!          0\n",
       "2     So the Scottish Government want people to get ...          1\n",
       "3     villainous pro tip : change the device name on...          0\n",
       "4         I would date any of these men :pleading_face:          0\n",
       "...                                                 ...        ...\n",
       "1395  I’ve just seen this and felt it deserved a Ret...          0\n",
       "1396     Omg how an earth is that a pen !! :clown_face:          0\n",
       "1397          Bringing Kanye and drake to a tl near you          0\n",
       "1398  I love it when women are referred to as \"girl ...          1\n",
       "1399  The fact that people still don't get that you ...          1\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_En_test = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/TaskA_Test.csv\")\n",
    "df_test = df_En_test.dropna(subset=['tweet','sarcastic'])\n",
    "df_test =df_test.dropna(axis=1,how='any')\n",
    "df_test =df_test.reset_index(drop=True)\n",
    "df_test['tweet']=df_test['tweet'].apply(clearTextFunc)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HL_test = pd.read_csv(\"/home/yang/NLP_train/classificationWithBERT/Sarcasm_Headlines_Dataset_v2.csv\")\n",
    "df_HL_test = df_HL_test.drop('article_link',axis=1)\n",
    "df_HL_test = df_HL_test.rename(columns={'is_sarcastic':'sarcastic', 'headline':'tweet'})\n",
    "\n",
    "df_HL_test['tweet']=df_HL_test['tweet'].apply(clearTextFunc)\n",
    "df_test=df_HL_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader=create_data_loader(df_test, tokenizer,MAX_LEN,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5421428571428571"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc,_ = eval_model(\n",
    "    model, test_data_loader, loss_fn, device, len(df_test)\n",
    ")\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    tweet_texts=[]\n",
    "    predictions=[]\n",
    "    prediction_probs=[]\n",
    "    real_values=[]\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts=d['tweet_text']\n",
    "            input_ids=d['input_ids'].to(device)\n",
    "            attention_mask=d['attention_mask'].to(device)\n",
    "            targets=d['targets'].to(device)\n",
    "\n",
    "            outputs=model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds=torch.max(outputs,dim=1)\n",
    "            probs = F.softmax(outputs,dim=1)\n",
    "            tweet_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions=torch.stack(predictions).cpu()\n",
    "    prediction_probs=torch.stack(prediction_probs).cpu()\n",
    "    real_values=torch.stack(real_values).cpu()\n",
    "    return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(model, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.7457    0.6293    0.6825       410\n",
      "    sarcastic     0.6793    0.7854    0.7285       410\n",
      "\n",
      "     accuracy                         0.7073       820\n",
      "    macro avg     0.7125    0.7073    0.7055       820\n",
      " weighted avg     0.7125    0.7073    0.7055       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred,target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/yang/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs,y_test = get_predictions(\n",
    "    model, test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.8824    0.5375    0.6680      1200\n",
      "    sarcastic     0.1704    0.5700    0.2624       200\n",
      "\n",
      "     accuracy                         0.5421      1400\n",
      "    macro avg     0.5264    0.5537    0.4652      1400\n",
      " weighted avg     0.7806    0.5421    0.6101      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=class_names,digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f39ef18d35d925f5763d2a3e564ffd5568da5f87fa68eaddb65a9b6abdd95fcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
